# model_manager.py
import os
import joblib
import pandas as pd
import numpy as np
from xgboost import XGBClassifier
import streamlit as st

class ModelManager:
    def __init__(self):
        self.model_paths = [
            'models/enhanced_fraud_model.joblib',
            'models/deployment_model.joblib', 
            'models/fallback_model.joblib'
        ]
        self.ensure_models_directory()
    
    def ensure_models_directory(self):
        """Create models directory if it doesn't exist"""
        os.makedirs('models', exist_ok=True)
    
    def create_deployment_model(self):
        """Create a robust model specifically for deployment"""
        print("üîß Creating deployment-ready ML model...")
        
        # Larger, more realistic dataset
        n_samples = 20000
        np.random.seed(42)
        
        # Realistic feature ranges based on your actual data patterns
        data = {
            'cc_num': np.random.randint(10000000, 99999999, n_samples),
            'amt': np.concatenate([
                np.random.exponential(50, n_samples//2),  # Normal transactions
                np.random.uniform(500, 5000, n_samples//2)  # High-value transactions
            ]),
            'lat': 40.7 + np.random.normal(0, 0.3, n_samples),
            'long': -74.0 + np.random.normal(0, 0.3, n_samples),
            'merch_lat': np.concatenate([
                40.7 + np.random.normal(0, 0.5, n_samples//2),  # Local
                25.0 + np.random.normal(0, 10.0, n_samples//4),  # International
                34.0 + np.random.normal(0, 5.0, n_samples//4)   # Domestic far
            ]),
            'merch_long': np.concatenate([
                -74.0 + np.random.normal(0, 0.5, n_samples//2),  # Local
                55.0 + np.random.normal(0, 15.0, n_samples//4),  # International
                -118.0 + np.random.normal(0, 8.0, n_samples//4)  # Domestic far
            ]),
            'hour': np.random.randint(0, 24, n_samples),
            'city_pop': np.random.choice([500000, 1000000, 2000000, 8419000], n_samples),
            'high_risk_hour': np.random.choice([0, 1], n_samples, p=[0.7, 0.3])
        }
        
        X = pd.DataFrame(data)
        
        # Calculate geographic distance
        X['geo_distance'] = np.sqrt(
            (X['lat'] - X['merch_lat'])**2 + (X['long'] - X['merch_long'])**2
        )
        
        # Realistic fraud patterns (matching your enhanced model)
        fraud_conditions = (
            (X['amt'] > 1000) & (X['geo_distance'] > 3.0) |  # High amount + distance
            (X['amt'] > 2000) |  # Very high amount
            (X['geo_distance'] > 8.0) |  # Extreme distance
            (X['high_risk_hour'] == 1) & (X['amt'] > 500)  # High risk hour + amount
        )
        
        y = fraud_conditions.astype(int)
        
        # Add category features (14 categories like your actual model)
        categories = ['entertainment', 'food_dining', 'gas_transport', 'grocery_net', 'grocery_pos',
                     'health_fitness', 'home', 'kids_pets', 'misc_net', 'misc_pos', 
                     'personal_care', 'shopping_net', 'shopping_pos', 'travel']
        
        for i, cat in enumerate(categories):
            X[f'cat_{cat}'] = np.random.choice([0, 1], n_samples, p=[0.9, 0.1])
        
        # Scale amount like your actual preprocessing
        X['amt_scaled'] = (X['amt'] - 70.0) / 200.0
        
        # Select final features (29 features like your enhanced model)
        feature_columns = [
            'cc_num', 'amt_scaled', 'lat', 'long', 'city_pop', 'merch_lat', 'merch_long',
            'hour', 'high_risk_hour', 'geo_distance'
        ] + [f'cat_{cat}' for cat in categories]
        
        X_final = X[feature_columns]
        
        # Train robust model
        model = XGBClassifier(
            n_estimators=100,
            max_depth=8,
            learning_rate=0.1,
            subsample=0.8,
            colsample_bytree=0.8,
            scale_pos_weight=len(y[y==0]) / len(y[y==1]),
            random_state=42,
            eval_metric='auc'
        )
        
        model.fit(X_final, y)
        
        # Save deployment model
        model_data = {
            'model': model,
            'feature_columns': feature_columns,
            'training_date': pd.Timestamp.now(),
            'model_type': 'deployment_enhanced'
        }
        
        joblib.dump(model_data, 'models/deployment_model.joblib')
        print("‚úÖ Deployment model created and saved with enhanced patterns")
        
        # Test the model
        from sklearn.metrics import roc_auc_score
        y_pred_proba = model.predict_proba(X_final)[:, 1]
        auc = roc_auc_score(y, y_pred_proba)
        print(f"üìä Deployment Model AUC: {auc:.4f}")
        
        return model_data
    
    def load_or_create_model(self):
        """Load existing model or create new one for deployment"""
        # Try all model paths
        for model_path in self.model_paths:
            try:
                if os.path.exists(model_path):
                    model_data = joblib.load(model_path)
                    print(f"‚úÖ Model loaded from {model_path}")
                    
                    # Validate model has required attributes
                    if hasattr(model_data, 'predict_proba') or ('model' in model_data and hasattr(model_data['model'], 'predict_proba')):
                        return model_data
                    else:
                        print(f"‚ùå Invalid model format in {model_path}")
            except Exception as e:
                print(f"‚ùå Failed to load {model_path}: {e}")
                continue
        
        # If no model exists, create deployment model
        print("üöÄ Creating new deployment model...")
        return self.create_deployment_model()

# Global model instance
_model_manager = ModelManager()

@st.cache_resource
def get_ml_model():
    """Streamlit cached resource to get ML model"""
    return _model_manager.load_or_create_model()